{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "For this assignment you will use the `wine.csv` data with the goal of building a red or white wine classifier. Use all the features in the dataset, allowing the network to decide how to build the internal weighting system.\n",
    "\n",
    "1. Load the `wine.csv` data and prepare the data for analysis: Split the data into training and testing and normalize the features. <span style=\"color:red\" float:right>[3 point]</span>\n",
    "1. Train a logistic regression classifier to predict the type of wine (red vs. white). Report the accuracy of the model. <span style=\"color:red\" float:right>[5 point]</span>\n",
    "1. Train a multi-layer feed-forward neural network to predict the type of wine. You network should have one hidden layer. You are free to choose how many neurons you want in the hidden layer. <span style=\"color:red\" float:right>[15 point]</span>\n",
    "1. Tune your neural network by trying different values for the learning rate and the number of neurons in the hidden layer. <span style=\"color:red\" float:right>[10 point]</span>\n",
    "1. Report the accuracy of the best model you obtained in the previous step. <span style=\"color:red\" float:right>[5 point]</span>\n",
    "\n",
    "Determine what the best neural network structure and hyperparameter settings results in the best predictive capability\n",
    "\n",
    "# End of assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, mean_squared_error, precision_recall_fscore_support\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the wine.csv data and prepare the data for analysis: Split the data into training and testing and normalize the features. [3 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixed acidity           float64\n",
      "volatile acidity        float64\n",
      "citric acid             float64\n",
      "residual sugar          float64\n",
      "chlorides               float64\n",
      "free sulfur dioxide     float64\n",
      "total sulfur dioxide    float64\n",
      "density                 float64\n",
      "pH                      float64\n",
      "sulphates               float64\n",
      "alcohol                 float64\n",
      "quality                   int64\n",
      "Class                     int64\n",
      "dtype: object\n",
      "fixed acidity           0\n",
      "volatile acidity        0\n",
      "citric acid             0\n",
      "residual sugar          0\n",
      "chlorides               0\n",
      "free sulfur dioxide     0\n",
      "total sulfur dioxide    0\n",
      "density                 0\n",
      "pH                      0\n",
      "sulphates               0\n",
      "alcohol                 0\n",
      "quality                 0\n",
      "Class                   0\n",
      "dtype: int64\n",
      "Accuracy of Logistic Regression is 0.9917948717948718\n"
     ]
    }
   ],
   "source": [
    "#load data and perform EDA\n",
    "wine = pd.read_csv('wine.csv')\n",
    "print(wine.dtypes)\n",
    "print(wine.isna().sum())\n",
    "wine.head()\n",
    "\n",
    "#split data into train and test sets\n",
    "seed = 24\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.drop(columns = \"Class\"), wine[\"Class\"], \n",
    "                                                    test_size = 0.30, random_state = seed)\n",
    "\n",
    "X_train = X_train.reset_index(drop = True)\n",
    "X_test = X_test.reset_index(drop = True)\n",
    "\n",
    "#normalization\n",
    "znormalizer = StandardScaler()\n",
    "znormalizer.fit(X_train)\n",
    "X_train_norm = pd.DataFrame(znormalizer.transform(X_train))\n",
    "X_test_norm = pd.DataFrame(znormalizer.transform(X_test))\n",
    "\n",
    "#logistic regression model\n",
    "logit = LogisticRegression()\n",
    "logit.fit(X_train_norm, y_train)\n",
    "y_pred_logit = logit.predict(X_test_norm)\n",
    "print(\"Accuracy of Logistic Regression is {}\".format(accuracy_score(y_test, y_pred_logit)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                832       \n",
      "=================================================================\n",
      "Total params: 832\n",
      "Trainable params: 832\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/40\n",
      "285/285 [==============================] - 1s 1ms/step - loss: 1.2369 - accuracy: 0.0222\n",
      "Epoch 2/40\n",
      "285/285 [==============================] - 0s 1ms/step - loss: 0.3858 - accuracy: 0.0477\n",
      "Epoch 3/40\n",
      "285/285 [==============================] - 0s 1ms/step - loss: 0.3835 - accuracy: 0.0561\n",
      "Epoch 4/40\n",
      "285/285 [==============================] - 0s 1ms/step - loss: 0.3356 - accuracy: 0.0719\n",
      "Epoch 5/40\n",
      "285/285 [==============================] - 0s 1ms/step - loss: 0.3407 - accuracy: 0.0901\n",
      "Epoch 6/40\n",
      "285/285 [==============================] - 0s 1ms/step - loss: 0.3066 - accuracy: 0.0958\n",
      "Epoch 7/40\n",
      "285/285 [==============================] - 0s 1ms/step - loss: 0.3004 - accuracy: 0.1055\n",
      "Epoch 8/40\n",
      "285/285 [==============================] - 0s 1ms/step - loss: 0.2937 - accuracy: 0.1056\n",
      "Epoch 9/40\n",
      "285/285 [==============================] - 0s 1ms/step - loss: 0.2708 - accuracy: 0.1075\n",
      "Epoch 10/40\n",
      "285/285 [==============================] - 0s 1ms/step - loss: 0.2816 - accuracy: 0.1280\n",
      "Epoch 11/40\n",
      "285/285 [==============================] - 0s 1ms/step - loss: 0.2337 - accuracy: 0.2087\n",
      "Epoch 12/40\n",
      "285/285 [==============================] - 0s 1ms/step - loss: 0.2067 - accuracy: 0.2851\n",
      "Epoch 13/40\n",
      "285/285 [==============================] - 0s 1ms/step - loss: 0.1926 - accuracy: 0.2858\n",
      "Epoch 14/40\n",
      "285/285 [==============================] - 0s 1ms/step - loss: 0.1921 - accuracy: 0.3067\n",
      "Epoch 15/40\n",
      "285/285 [==============================] - 0s 1ms/step - loss: 0.1803 - accuracy: 0.3221\n",
      "Epoch 16/40\n",
      "285/285 [==============================] - 0s 1ms/step - loss: 0.1919 - accuracy: 0.3351\n",
      "Epoch 17/40\n",
      "285/285 [==============================] - 0s 1ms/step - loss: 0.1837 - accuracy: 0.3356\n",
      "Epoch 18/40\n",
      "285/285 [==============================] - 0s 1ms/step - loss: 0.1987 - accuracy: 0.3628\n",
      "Epoch 19/40\n",
      "285/285 [==============================] - 0s 1ms/step - loss: 0.1507 - accuracy: 0.3980\n",
      "Epoch 20/40\n",
      "285/285 [==============================] - 0s 1ms/step - loss: 0.1686 - accuracy: 0.3979\n",
      "Epoch 21/40\n",
      "285/285 [==============================] - 0s 1ms/step - loss: 0.1486 - accuracy: 0.4238\n",
      "Epoch 22/40\n",
      "285/285 [==============================] - 0s 1ms/step - loss: 0.1531 - accuracy: 0.4298\n",
      "Epoch 23/40\n",
      "285/285 [==============================] - 0s 1ms/step - loss: 0.1413 - accuracy: 0.4398\n",
      "Epoch 24/40\n",
      "285/285 [==============================] - 0s 999us/step - loss: 0.1359 - accuracy: 0.4538\n",
      "Epoch 25/40\n",
      "285/285 [==============================] - 0s 988us/step - loss: 0.1421 - accuracy: 0.4759\n",
      "Epoch 26/40\n",
      "285/285 [==============================] - 0s 1ms/step - loss: 0.1334 - accuracy: 0.4764\n",
      "Epoch 27/40\n",
      "285/285 [==============================] - 0s 1ms/step - loss: 0.1349 - accuracy: 0.4722\n",
      "Epoch 28/40\n",
      "285/285 [==============================] - 0s 1ms/step - loss: 0.1407 - accuracy: 0.4838\n",
      "Epoch 29/40\n",
      "285/285 [==============================] - 0s 1ms/step - loss: 0.1346 - accuracy: 0.4911\n",
      "Epoch 30/40\n",
      "285/285 [==============================] - 0s 1ms/step - loss: 0.1262 - accuracy: 0.4911\n",
      "Epoch 31/40\n",
      "285/285 [==============================] - 0s 1ms/step - loss: 0.1140 - accuracy: 0.4860\n",
      "Epoch 32/40\n",
      "285/285 [==============================] - 0s 1ms/step - loss: 0.1199 - accuracy: 0.5015\n",
      "Epoch 33/40\n",
      "285/285 [==============================] - 0s 1ms/step - loss: 0.1278 - accuracy: 0.5044\n",
      "Epoch 34/40\n",
      "285/285 [==============================] - 0s 1ms/step - loss: 0.1143 - accuracy: 0.5214\n",
      "Epoch 35/40\n",
      "285/285 [==============================] - 0s 1ms/step - loss: 0.1024 - accuracy: 0.5164\n",
      "Epoch 36/40\n",
      "285/285 [==============================] - 0s 1ms/step - loss: 0.1359 - accuracy: 0.5257\n",
      "Epoch 37/40\n",
      "285/285 [==============================] - 0s 1ms/step - loss: 0.1165 - accuracy: 0.5277\n",
      "Epoch 38/40\n",
      "285/285 [==============================] - 0s 1ms/step - loss: 0.1179 - accuracy: 0.5343\n",
      "Epoch 39/40\n",
      "285/285 [==============================] - 0s 1ms/step - loss: 0.1101 - accuracy: 0.5464\n",
      "Epoch 40/40\n",
      "285/285 [==============================] - 0s 1ms/step - loss: 0.0992 - accuracy: 0.5462\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(64, input_dim=X_train_norm.shape[1], activation=\"relu\"), # first hidden layer \n",
    "])\n",
    "\n",
    "# summary of model object\n",
    "print(model.summary())\n",
    "\n",
    "#model compile\n",
    "model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.1, momentum=0.0), \n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "#train model\n",
    "pred = model.fit(X_train_norm, \n",
    "                    y_train, \n",
    "                    batch_size=16,\n",
    "                    epochs=40,\n",
    "                    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-bfa5102d52d3>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-bfa5102d52d3>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.1, momentum=0.0),\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#tuning with learning rate and num of neurons in hidden layer\n",
    "learning_rates = [0.001,0.01,0.1]\n",
    "num_neurons = [64,32,8]\n",
    "\n",
    "for rate in learning_rates:\n",
    "    for neurons in num_neurons:\n",
    "        model = keras.Sequential([keras.layers.Dense(neurons, input_dim=X_train_norm.shape[1], activation=\"relu\"), # first hidden layer])\n",
    "\n",
    "        # summary of model object\n",
    "        print(model.summary())\n",
    "\n",
    "        #model compile\n",
    "        model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.1, momentum=0.0), \n",
    "                      loss=\"binary_crossentropy\",\n",
    "                      metrics=[\"accuracy\"])\n",
    "\n",
    "        #train model\n",
    "        pred = model.fit(X_train_norm, \n",
    "                            y_train, \n",
    "                            batch_size=16,\n",
    "                            epochs=40,\n",
    "                            verbose=1\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
